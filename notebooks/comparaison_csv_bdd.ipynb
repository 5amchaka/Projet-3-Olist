{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparaison CSV bruts vs BDD star schema\n",
    "\n",
    "Ce notebook trace le parcours des donnees depuis les 9 CSV bruts jusqu'aux 6 tables du star schema.\n",
    "Trois angles d'analyse :\n",
    "\n",
    "1. **Volumetrie** — combien de lignes/colonnes entrent, combien sortent\n",
    "2. **Distributions avant/apres** — les transformations preservent-elles la coherence ?\n",
    "3. **Perdu / Gagne** — ce qui disparait, ce qui est cree par l'ETL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import warnings\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from src.config import DATABASE_PATH\n",
    "from src.etl.extract import load_all_raw\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des CSV bruts\n",
    "raw = load_all_raw()\n",
    "\n",
    "# Chargement des tables du star schema\n",
    "conn = sqlite3.connect(str(DATABASE_PATH))\n",
    "db = {}\n",
    "for table in [\"dim_dates\", \"dim_geolocation\", \"dim_customers\", \"dim_sellers\", \"dim_products\", \"fact_orders\"]:\n",
    "    db[table] = pd.read_sql(f\"SELECT * FROM {table}\", conn)\n",
    "conn.close()\n",
    "\n",
    "print(f\"CSV : {len(raw)} datasets\")\n",
    "print(f\"BDD : {len(db)} tables\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Volumetrie — le bilan des transformations\n",
    "\n",
    "Combien de lignes et colonnes avant/apres l'ETL ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping CSV source → table cible\n",
    "mapping = {\n",
    "    \"dim_geolocation\": [\"geolocation\"],\n",
    "    \"dim_customers\": [\"customers\"],\n",
    "    \"dim_sellers\": [\"sellers\"],\n",
    "    \"dim_products\": [\"products\", \"category_translation\"],\n",
    "    \"dim_dates\": [\"orders\"],\n",
    "    \"fact_orders\": [\"order_items\", \"orders\", \"order_payments\", \"order_reviews\"],\n",
    "}\n",
    "\n",
    "rows = []\n",
    "for table, sources in mapping.items():\n",
    "    src_rows = sum(raw[s].shape[0] for s in sources)\n",
    "    src_cols = sum(raw[s].shape[1] for s in sources)\n",
    "    dst_rows = db[table].shape[0]\n",
    "    dst_cols = db[table].shape[1]\n",
    "    ratio = f\"{src_rows / dst_rows:.1f}:1\" if dst_rows > 0 else \"-\"\n",
    "    rows.append({\n",
    "        \"Table cible\": table,\n",
    "        \"CSV sources\": \" + \".join(sources),\n",
    "        \"Lignes CSV\": src_rows,\n",
    "        \"Lignes BDD\": dst_rows,\n",
    "        \"Ratio\": ratio,\n",
    "        \"Colonnes CSV\": src_cols,\n",
    "        \"Colonnes BDD\": dst_cols,\n",
    "    })\n",
    "\n",
    "vol = pd.DataFrame(rows).set_index(\"Table cible\")\n",
    "vol.style.format({\"Lignes CSV\": \"{:,}\", \"Lignes BDD\": \"{:,}\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du ratio lignes CSV → BDD\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "x = range(len(vol))\n",
    "width = 0.35\n",
    "\n",
    "bars_csv = ax.bar([i - width/2 for i in x], vol[\"Lignes CSV\"], width, label=\"CSV bruts\", color=\"steelblue\")\n",
    "bars_bdd = ax.bar([i + width/2 for i in x], vol[\"Lignes BDD\"], width, label=\"BDD star schema\", color=\"coral\")\n",
    "\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(vol.index, rotation=30, ha=\"right\")\n",
    "ax.set_ylabel(\"Nombre de lignes\")\n",
    "ax.set_yscale(\"log\")\n",
    "ax.set_title(\"Volume de lignes : CSV bruts vs BDD (echelle log)\")\n",
    "ax.legend()\n",
    "\n",
    "# Annotations ratio\n",
    "for i, ratio in enumerate(vol[\"Ratio\"]):\n",
    "    ax.annotate(ratio, (i, max(vol.iloc[i][\"Lignes CSV\"], vol.iloc[i][\"Lignes BDD\"]) * 1.3),\n",
    "                ha=\"center\", fontsize=9, fontweight=\"bold\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lecture du bilan volumetrique\n",
    "\n",
    "- **geolocation** subit la compression la plus forte (~53:1) : deduplication par zip_code_prefix\n",
    "- **dim_dates** est **generee** a partir des timestamps de orders — le ratio n'a pas de sens direct\n",
    "- **fact_orders** fusionne 4 CSV mais conserve le grain order_item (112k lignes) — les paiements et reviews sont agreges, pas les items\n",
    "- **customers, sellers, products** restent proches du 1:1 — leur grain etait deja correct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Distributions avant/apres\n",
    "\n",
    "Les transformations preservent-elles les distributions des donnees ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 Geolocation — coordonnees brutes vs medianes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "geo_raw = raw[\"geolocation\"]\ngeo_db = db[\"dim_geolocation\"]\n\n# Bornes geographiques du Bresil — exclut les outliers GPS\nLAT_BOUNDS = (-35, 6)\nLNG_BOUNDS = (-75, -34)\n\nraw_lat = geo_raw[\"geolocation_lat\"]\nraw_lng = geo_raw[\"geolocation_lng\"]\ndb_lat = geo_db[\"lat\"].dropna()\ndb_lng = geo_db[\"lng\"].dropna()\n\n# Filtrage\nraw_lat_f = raw_lat[raw_lat.between(*LAT_BOUNDS)]\nraw_lng_f = raw_lng[raw_lng.between(*LNG_BOUNDS)]\ndb_lat_f = db_lat[db_lat.between(*LAT_BOUNDS)]\ndb_lng_f = db_lng[db_lng.between(*LNG_BOUNDS)]\n\noutliers_raw = len(raw_lat) - len(raw_lat_f)\noutliers_db = len(db_lat) - len(db_lat_f)\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5))\n\n# Latitude — bins partages\naxes[0].hist(raw_lat_f, bins=80, alpha=0.5, label=f\"CSV brut ({len(geo_raw):,})\", color=\"steelblue\", density=True)\naxes[0].hist(db_lat_f, bins=80, alpha=0.6, label=f\"BDD mediane ({len(geo_db):,})\", color=\"coral\", density=True)\naxes[0].set_title(\"Distribution des latitudes\")\naxes[0].set_xlabel(\"Latitude\")\naxes[0].set_xlim(LAT_BOUNDS)\naxes[0].legend(fontsize=8)\n\n# Longitude — bins partages\naxes[1].hist(raw_lng_f, bins=80, alpha=0.5, label=\"CSV brut\", color=\"steelblue\", density=True)\naxes[1].hist(db_lng_f, bins=80, alpha=0.6, label=\"BDD mediane\", color=\"coral\", density=True)\naxes[1].set_title(\"Distribution des longitudes\")\naxes[1].set_xlabel(\"Longitude\")\naxes[1].set_xlim(LNG_BOUNDS)\naxes[1].legend(fontsize=8)\n\nplt.suptitle(\"Geolocation : 1M lignes brutes vs 19k medianes par zip (bornes Bresil)\", fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n\nprint(f\"Outliers exclus — CSV: {outliers_raw}, BDD: {outliers_db}\")\nprint(\"→ La forme globale est preservee. L'agregation par mediane lisse les outliers\")\nprint(\"  sans deformer la distribution geographique sous-jacente.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Paiements — valeurs brutes vs agregees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "pays_raw = raw[\"order_payments\"]\nfact = db[\"fact_orders\"]\n\n# payment_value brut (par ligne de paiement) vs order_payment_total (agrege par commande)\npays_agg_raw = pays_raw.groupby(\"order_id\")[\"payment_value\"].sum()\npays_agg_db = fact.drop_duplicates(subset=\"order_id\")[\"order_payment_total\"].dropna()\n\nUPPER = 500\nfig, axes = plt.subplots(1, 3, figsize=(16, 4))\n\n# Distribution brute par ligne (filtree, pas clippee)\npays_raw.loc[pays_raw[\"payment_value\"] <= UPPER, \"payment_value\"].plot.hist(\n    bins=50, ax=axes[0], color=\"steelblue\", edgecolor=\"white\")\naxes[0].set_title(f\"CSV brut : payment_value par ligne\\n({len(pays_raw):,} lignes)\")\naxes[0].set_xlabel(f\"BRL (≤ {UPPER})\")\n\n# Distribution agregee depuis CSV\npays_agg_raw[pays_agg_raw <= UPPER].plot.hist(\n    bins=50, ax=axes[1], color=\"mediumseagreen\", edgecolor=\"white\")\naxes[1].set_title(f\"CSV agrege : sum par commande\\n({len(pays_agg_raw):,} commandes)\")\naxes[1].set_xlabel(f\"BRL (≤ {UPPER})\")\n\n# Distribution dans la BDD\npays_agg_db[pays_agg_db <= UPPER].plot.hist(\n    bins=50, ax=axes[2], color=\"coral\", edgecolor=\"white\")\naxes[2].set_title(f\"BDD fact_orders : order_payment_total\\n({len(pays_agg_db):,} commandes uniques)\")\naxes[2].set_xlabel(f\"BRL (≤ {UPPER})\")\n\nplt.suptitle(\"Paiements : de la ligne brute au total par commande\", fontweight=\"bold\")\nplt.tight_layout()\nplt.show()\n\npct_excl = (pays_agg_raw > UPPER).mean()\nprint(f\"Commandes > {UPPER} BRL exclues de la visu : {pct_excl:.1%}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Verification numerique : les totaux correspondent-ils ?\nitems_raw = raw[\"order_items\"]\n\ntotal_csv = pays_raw[\"payment_value\"].sum()\ntotal_db = fact.drop_duplicates(subset=\"order_id\")[\"order_payment_total\"].sum()\n\nprint(f\"Total paiements CSV brut   : {total_csv:,.2f} BRL\")\nprint(f\"Total paiements BDD (dedup): {total_db:,.2f} BRL\")\nprint(f\"Ecart                      : {abs(total_csv - total_db):,.2f} BRL ({abs(total_csv - total_db)/total_csv:.4%})\")\n\nif abs(total_csv - total_db) < 1:\n    print(\"\\n→ Les totaux paiements concordent parfaitement.\")\nelse:\n    print(f\"\\n→ Ecart de {abs(total_csv - total_db):,.2f} BRL — a investiguer.\")\n\n# Verification prix et freight (order_items)\nprint()\nfor col in [\"price\", \"freight_value\"]:\n    csv_sum = items_raw[col].sum()\n    db_sum = fact[col].sum()\n    print(f\"{col:15s} — CSV: {csv_sum:>12,.2f}  BDD: {db_sum:>12,.2f}  ecart: {abs(csv_sum - db_sum):.2f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Perdu / Gagne — ce que l'ETL transforme\n",
    "\n",
    "Bilan de ce qui disparait des CSV bruts et de ce que le star schema apporte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Colonnes source (CSV) vs colonnes cible (BDD) par table\nfor table, sources in mapping.items():\n    csv_cols = []\n    for s in sources:\n        csv_cols.extend([f\"{s}.{c}\" for c in raw[s].columns])\n    bdd_cols = list(db[table].columns)\n\n    print(f\"{'─'*60}\")\n    print(f\"  {table}  ←  {' + '.join(sources)}\")\n    print(f\"  CSV : {len(csv_cols)} colonnes  →  BDD : {len(bdd_cols)} colonnes\")\n    print(f\"  CSV : {', '.join(c.split('.')[-1] for c in csv_cols)}\")\n    print(f\"  BDD : {', '.join(bdd_cols)}\")\n    print()\n\n# Bilan global\nall_csv = sum(raw[s].shape[1] for s in raw)\nall_bdd = sum(db[t].shape[1] for t in db)\nprint(f\"{'='*60}\")\nprint(f\"  Total : {all_csv} colonnes CSV  →  {all_bdd} colonnes BDD\")"
  },
  {
   "cell_type": "markdown",
   "source": "### Lecture du bilan\n\n**Colonnes perdues** — supprimees car a faible valeur analytique :\n- Textes creux : `review_comment_title` (88% vide), `review_comment_message` (59% vide)\n- Identifiants techniques : `review_id`, `payment_sequential`\n- Timestamps intermediaires : `order_approved_at`, `order_delivered_carrier_date`, etc. (utilises pour calculer `delivery_days` puis supprimes)\n- Metadonnees derivees : `product_name_lenght`, `product_description_lenght`, `shipping_limit_date`\n- Multi-lignes geolocation : 1M lignes reduites a 19k par agregation mediane/mode\n\n**Colonnes gagnees** — creees par l'ETL :\n- Cles surrogate et FK (`*_key`, `date_key`, `geo_key`) pour les jointures du star schema\n- Metriques pre-calculees : `order_payment_total`, `delivery_days`, `estimated_days`, `delivery_delta_days`\n- Traduction : `category_name_en` (fusion avec `category_translation`)\n- Dimension temporelle : table `dim_dates` entierement generee (year, quarter, month, is_weekend)",
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n## 4. Synthese\n\n### Bilan de tracabilite\n\n| Aspect | Constat |\n|--------|--------|\n| **Volumetrie** | 1.3M lignes CSV → 268k lignes BDD. La compression vient essentiellement de geolocation (53:1) et de l'agregation paiements/reviews |\n| **Integrite numerique** | Les totaux price, freight et payment concordent entre CSV et BDD — aucune perte de donnees financieres |\n| **Distributions** | Les distributions clefs (coordonnees, paiements) sont preservees apres transformation |\n| **Colonnes perdues** | 16 colonnes a faible valeur analytique (textes vides, timestamps bruts, metadonnees) |\n| **Colonnes gagnees** | 19 colonnes utiles (cles surrogate, FK, metriques derivees, traductions, dimension temporelle) |\n\n### Conclusion\n\nLe pipeline ETL transforme 9 CSV en 6 tables sans perte d'information analytique.\nLes donnees supprimees sont soit redondantes (multi-lignes geolocation), soit a faible valeur (textes vides, metadonnees).\nEn retour, le star schema apporte des cles de jointure propres, des metriques pre-calculees et une dimension temporelle complete.\n\nVoir aussi :\n- [`docs/csv_to_star_schema.md`](../docs/csv_to_star_schema.md) — les choix de modelisation\n- [`docs/exploration_analysis.md`](../docs/exploration_analysis.md) — l'analyse empirique des CSV bruts\n- [`notebooks/exploration_csv.ipynb`](exploration_csv.ipynb) — l'exploration detaillee des CSV"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}