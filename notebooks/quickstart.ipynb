{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quickstart — Pipeline ETL Olist\n",
    "\n",
    "Ce notebook reproduit les etapes du README pour lancer le pipeline ETL de bout en bout :\n",
    "1. Verification de l'environnement\n",
    "2. Telechargement des donnees Kaggle\n",
    "3. Execution du pipeline ETL\n",
    "4. Verification de la base de donnees\n",
    "5. Requetes exploratoires\n",
    "\n",
    "**Pre-requis** : `uv venv && uv sync` et credentials Kaggle dans `.env`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & verifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sqlite3\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from src.config import CSV_FILES, DATABASE_PATH, RAW_DIR\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s | %(name)s | %(message)s\")\n",
    "\n",
    "# Verifier .env\n",
    "env_path = Path(\"..\") / \".env\"\n",
    "if env_path.exists():\n",
    "    print(\".env trouve\")\n",
    "else:\n",
    "    print(\".env manquant — copier .env.example en .env et renseigner KAGGLE_USERNAME / KAGGLE_KEY\")\n",
    "\n",
    "# Verifier les CSV\n",
    "missing = [name for name, fname in CSV_FILES.items() if not (RAW_DIR / fname).exists()]\n",
    "if missing:\n",
    "    print(f\"CSV manquants ({len(missing)}/{len(CSV_FILES)}) : {missing}\")\n",
    "    print(\"Lancer la cellule suivante pour telecharger les donnees.\")\n",
    "else:\n",
    "    print(f\"Les {len(CSV_FILES)} CSV sont presents dans {RAW_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Telechargement des donnees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Telecharger depuis Kaggle (skip si deja present)\n",
    "missing = [name for name, fname in CSV_FILES.items() if not (RAW_DIR / fname).exists()]\n",
    "if missing:\n",
    "    !bash ../scripts/download_dataset.sh\n",
    "else:\n",
    "    print(\"Donnees deja presentes, telechargement ignore.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Lancer le pipeline ETL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.etl.pipeline import run_full_pipeline\n",
    "\n",
    "run_full_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Verification de la base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TABLES = [\n",
    "    \"dim_dates\",\n",
    "    \"dim_geolocation\",\n",
    "    \"dim_customers\",\n",
    "    \"dim_sellers\",\n",
    "    \"dim_products\",\n",
    "    \"fact_orders\",\n",
    "]\n",
    "\n",
    "conn = sqlite3.connect(DATABASE_PATH)\n",
    "\n",
    "# Volumetrie\n",
    "counts = {t: pd.read_sql(f\"SELECT COUNT(*) AS n FROM {t}\", conn)[\"n\"].iloc[0] for t in TABLES}\n",
    "df_counts = pd.DataFrame(counts.items(), columns=[\"table\", \"lignes\"])\n",
    "display(df_counts)\n",
    "\n",
    "# Apercu de chaque table\n",
    "for t in TABLES:\n",
    "    print(f\"\\n--- {t} ---\")\n",
    "    display(pd.read_sql(f\"SELECT * FROM {t} LIMIT 3\", conn))\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Requetes exploratoires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "conn = sqlite3.connect(DATABASE_PATH)\n\n# Top 5 categories par nombre de commandes\nprint(\"Top 5 categories produits\")\ndisplay(pd.read_sql(\"\"\"\n    SELECT p.category_name_en AS categorie, COUNT(*) AS nb_commandes\n    FROM fact_orders f\n    JOIN dim_products p ON f.product_key = p.product_key\n    GROUP BY categorie\n    ORDER BY nb_commandes DESC\n    LIMIT 5\n\"\"\", conn))\n\n# Commandes par mois\nprint(\"\\nCommandes par mois\")\ndisplay(pd.read_sql(\"\"\"\n    SELECT d.year, d.month, COUNT(*) AS nb_commandes\n    FROM fact_orders f\n    JOIN dim_dates d ON f.date_key = d.date_key\n    GROUP BY d.year, d.month\n    ORDER BY d.year, d.month\n\"\"\", conn))\n\n# Score review moyen\nprint(\"\\nScore review moyen\")\ndisplay(pd.read_sql(\"\"\"\n    SELECT ROUND(AVG(review_score), 2) AS score_moyen\n    FROM fact_orders\n    WHERE review_score IS NOT NULL\n\"\"\", conn))\n\nconn.close()"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}